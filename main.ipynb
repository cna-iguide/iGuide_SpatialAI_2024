{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net for Image Segmentation\n",
    "\n",
    "## Overview\n",
    "This notebook is the entry point for this project. \n",
    "\n",
    "## Structure\n",
    "- **[Setup and Import Necessary Packages](#setup-and-import-necessary-packages)**: Install and import the libraries and packages needed for the project.\n",
    "- **[Input File Directories, Load Data, and View a Sample of Data](#input-file-directories-load-data-and-view-a-sample-of-data)**: Load images and masks form the given directories, and view a sample of that data for verification.\n",
    "- **[Configure and Preprocess Data](#configure-and-preprocess-data)**: Configure and prepare the data for the U-Net.\n",
    "- **[Build and Train the U-Net Model](#build-and-train-the-u-net-model)**: Configure, build, and train the U-Net. \n",
    "- **[Model Evaluation](#model-evaluation)**: Evaluate the model and save the predictions if desired. \n",
    "\n",
    "\n",
    "## Usage\n",
    "This notebook assumes that the helper functions (`utils.ipynb`), and the U-Net model functions (`unet.ipynb`) are imported correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Import Necessary Packages\n",
    "\n",
    "In the following three code cells, we will set up our environment by installing the necessary packages and importing the required libraries, and then optionally setting our model for \"production\" or \"testing\". This setup includes custom utility functions, a U-Net model implementation, and setting the random seed.\n",
    "\n",
    "### Step 1: Install Required Packages\n",
    "\n",
    "First, we will use the `%pip` magic command to install the necessary packages from a `requirements.txt` file. This ensures that all dependencies are installed before we proceed with importing them.\n",
    "\n",
    "If you don't have the `requirements.txt` file, you can find it [here](https://github.com/cna-iguide/iGuide_SpatialAI_2024).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the necessary packages\n",
    "# Suppress already satisfied warning using: | find /V \"already satisfied\"\n",
    "%pip install -r requirements.txt | find /V \"already satisfied\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Import Necessary Packages\n",
    "\n",
    "Next, we will import the necessary packages and libraries for our project. This includes:\n",
    "\n",
    "- Standard libraries and packages\n",
    "- Custom utility functions from `utils.py`\n",
    "- U-Net model implementation and associated functions from `unet.py`\n",
    "\n",
    "These custom files contain functions that will be used throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import import_ipynb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import (\n",
    "    load_images_and_masks,\n",
    "    view_data,\n",
    "    preprocess_images_and_masks,\n",
    "    remap_mask_classes,\n",
    "    save_datasets,\n",
    "    load_datasets,\n",
    "    save_associated_files,\n",
    "    custom_warnings,\n",
    ")\n",
    "from unet import (\n",
    "    unet_model,\n",
    "    test_unet,\n",
    "    plot_losses,\n",
    "    save_predicted_masks,\n",
    "    iou_metric,\n",
    "    dice_coeff,\n",
    ")\n",
    "\n",
    "# Optional line to suppress unnecessary tensorflow warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "from tensorflow.keras import callbacks, optimizers\n",
    "import tensorflow as tf\n",
    "\n",
    "# Force the warnings library to use the custom warnings format\n",
    "warnings.formatwarning = custom_warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Setting to Production or Testing\n",
    "\n",
    "Finally for setup we can set our model to be in testing mode (`PRODUCTION = False`) or production mode (`PRODUCTION = True`). \n",
    "\n",
    "In testing mode, this will keep the random seed the same across all the necessary libraries and packages, so that the code will have consistently reproducible results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force all seed values to be the same. This should only be done when testing.\n",
    "PRODUCTION = False\n",
    "seed_value = 42\n",
    "if not PRODUCTION:\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input File Directories, Load Data, and View a Sample of Data\n",
    "\n",
    "In the following three code cells, we will specify the file directories for images, masks, and model, and then load the data from these directories using a custom function. As a check that the data has been loaded correctly, we then view a small sample of the data.\n",
    "\n",
    "### Step 4: Specify File Directories\n",
    "\n",
    "First, we need to input the file directories where our images, masks, and models are stored. These directories will be used to load the data required for training and evaluation. These directories should all be relative file paths to the current working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the necessary directories. These should be relative to the working directory\n",
    "images_dir = \"Datasets/Pala_Mesa_Roads_Rails/images\" \n",
    "masks_dir = \"Datasets/Pala_Mesa_Roads_Rails/labels\"\n",
    "model_dir = \"models\"\n",
    "model_name = \"trained_UNet_Pala_Mesa.keras\"\n",
    "model_path = Path(f\"{model_dir}/{model_name}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Load the Images and Masks\n",
    "\n",
    "The next code cell loads the images and masks using a custom function defined in `utils.ipynb`. You can find more details on this function there or [here](https://github.com/cna-iguide/iGuide_SpatialAI_2024). \n",
    "\n",
    "We have set the optional settings such as:\n",
    "- `file_ext`: specifying the file extension type of our data\n",
    "- `max_count`: the maximum number of images we would like to load in for our model\n",
    "- `trim_names`: an additional feature that attempts to trim the filenames if they are of a format with characters and underscores preceding a large number of digits. More details can be found in the `utils.ipynb` under the `load_images_and_masks` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images and masks\n",
    "images, masks, missing_masks, names_map = load_images_and_masks(\n",
    "    images_dir, masks_dir, file_ext=\"tif\", max_count=1000, trim_names=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: View a Sample of Data\n",
    "\n",
    "The next code cell lets us view a sample of our images and masks, to make sure that they have loaded as expected. Again this is a custom function that can be found in `utils.ipynb`. You can find more details on this function there or [here](https://github.com/cna-iguide/iGuide_SpatialAI_2024).\n",
    "\n",
    "We have set the optional settings such as:\n",
    "- `max_plots`: specifying the number of images (and masks) to plot\n",
    "- `max_cols`: specifying the number of columns to use for display\n",
    "- `randomize`: specifying to use a random subset of the loaded images (and their corresponding masks)\n",
    "- `colors`: displaying the images and masks in color (as opposed to greyscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View images and masks\n",
    "view_data(\n",
    "    images=images,\n",
    "    masks=masks,\n",
    "    max_plots=20,\n",
    "    max_cols=5,\n",
    "    randomize=True,\n",
    "    colors=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and Preprocess Data\n",
    "\n",
    "In the following three code cells, we will prepare our data for the U-Net mode. We will optionally remap classes, preprocess the data, and split it into training, validation, and test sets.\n",
    "\n",
    "### Step 7: Optionally Remap the Mask Classes\n",
    "\n",
    "You can optionally remap the mask classes down to a subset of the mask classes. This is done using a custom function that can be found in `utils.ipynb`. You can find more details on this function there or [here](https://github.com/cna-iguide/iGuide_SpatialAI_2024). \n",
    "\n",
    "An important feature of the `remap_mask_classes` function: There are instances where, if it finds some discrepancies in the inputs, it will ask the user what they would like to do. Please follow the prompts accordingly.\n",
    "\n",
    "We have set `REMAP_CLASSES = False`. If we chose to set it to `True`, then the `class_mapping` would map the old class index values to the new class index values. In the example, this maps old values `0,1,2,3` to new values `0,1`. We have also implemented the custom `view_data` function shown previously in [Step 6](#step-6-view-a-sample-of-data). Notice that in this case, only the masks were provided, since the images will remain unchanged. Please also note that the function outputs not only the new masks but the `num_classes` which can be used as an input in [Step 8](#step-8-preprocess-the-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional remapping of classes\n",
    "REMAP_CLASSES = False\n",
    "class_mapping = {0: 0, 1: 1, 2: 1, 3: 1} # keys are old classes, values are the new classes\n",
    "if REMAP_CLASSES:\n",
    "    masks, num_classes = remap_mask_classes(masks, class_mapping)\n",
    "    view_data(masks=masks, max_plots=20, max_cols=5, randomize=False, colors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Preprocess the Data\n",
    "\n",
    "Next, we need to ensure that the data is in the correct format for the U-Net. This is done using the custom function `preprocess_images_and_masks` that can be found in `utils.ipynb`. You can find more details on this function there or [here](https://github.com/cna-iguide/iGuide_SpatialAI_2024).\n",
    "We have set the optional settings such as:\n",
    "- `num_classes`: the number of classes we believe to be in our masks. It is alright if the number of classes is incorrect as the `preprocess_images_and_masks` is designed to handle that. Please note that if you do remap the mask classes using the `remap_mask_classes` function in the previous step, you should use the output `num_classes` as your input here. \n",
    "- `threshold`: setting the value to use in the case of a binary mask (a mask with only background and foreground classes). It already has a default of 0.5, if it isn't set. \n",
    "\n",
    "An important feature of the `preprocess_images_and_masks` function: There are instances where, if it finds some discrepancies in the inputs, it will ask the user what they would like to do. Please follow the prompts accordingly.\n",
    "\n",
    "The `preprocess_images_and_masks` will output some basic information about the images, masks, and the number of classes. This is a convenent checkpoint to verify that the data is formatted as expected. For example the number of images and masks should be the same, the height and width of the images should be the same, and the type of the data should be in a format usable by a Tensorflow model like float32. You can use the number of classes later in [Step 15](#step-15-test-the-trained-model) to label the classes (for display purposes only).\n",
    "\n",
    "In general the number of classes should match the the 4th dimension in the masks shape. However there are a handful of exceptions. In the case where there are initially 2 masks classes, the data will be mapped to a binary case (foreground and background) for faster data processing, and you will be notified in a print out. While we can still think of this as two classes, the U-Net understands it as a single class. Similarly if the data is already in binary format it will only be one class. When there is only one class, the 4th dimension is not needed. All of this is handled in the `preprocess_images_and_masks` function, and is accounted for in later functions as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data for the U-Net\n",
    "# Assign default values to the number of classes\n",
    "num_classes = 4\n",
    "preprocessed_images, preprocessed_masks, threshold, image_names, num_classes = (\n",
    "    preprocess_images_and_masks(\n",
    "        images, masks, num_classes=num_classes, threshold=0.5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Split the Data\n",
    "\n",
    "The final step of preparing the data for the U-Net, is to split the data into training, validation, and test sets. \n",
    "\n",
    "If you already have a saved dataset from a previous run, you can load that dataset in. The `load_datasets` function is defined in `utils.ipynb` and details can be found there or [here](https://github.com/cna-iguide/iGuide_SpatialAI_2024/). The `load_datasets` function assumes that the data is saved by the `save_datasets` which is also defined in `utils.ipynb` and whose details can also be found there or [here](https://github.com/cna-iguide/iGuide_SpatialAI_2024/). It assumes that the datasets are saved as `.npy` with the following names:\n",
    "- Training data: `\"images_training.npy\"`, `\"masks_training.npy\"`\n",
    "- Test data: `\"images_test.npy\"`, `\"masks_test.npy\"`\n",
    "- Validation data: `\"images_validation.npy\"`, `\"masks_validation.npy\"`\n",
    "\n",
    "Please note that if you do load in a previous dataset, you must ensure that the number of classes is correct, as there is no check after [Step 8](#step-8-preprocess-the-data) that verifies the number of classes. \n",
    "\n",
    "If you do not have datasets to load in then `LOAD_DATASETS` should be set to `False`, and the following will occur:\n",
    "\n",
    "First we split out 80% of the data for training, by setting the `test_size`variable to `0.2`. We also set the `random_state` to our `seed_value` from [Step 3](#step-3-setting-to-production-or-testing). If desired, this variable can be removed from the input. Note that the `train_test_split` function is a scikit learn function, and the details can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "\n",
    "We then apply the same function to the remaining 20% of the data and split it evenly into validation, and testing data sets. \n",
    "\n",
    "In both splits we include a list with the names of the images, so that we can keep the original image names, and use them later on in [Step 15](#step-15-test-the-trained-model). This is necessary, as `train_test_split` randomizes the order of the images. \n",
    "\n",
    "After the data is split, you can set `SAVE_DATASETS` to `True` to save the split datasets for later use using the `save_datasets` function. \n",
    "\n",
    "This step is very important as it ensures that our model will have test data that it has never seen before, which is important for a true test of the model's ability to predict the mask classes based on the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training, validation, and test\n",
    "LOAD_DATASETS = False\n",
    "load_dir = \"Datasets/Pala_Mesa_Roads_Rails\"\n",
    "\n",
    "SAVE_DATASETS = True\n",
    "save_dir = \"Datasets/Pala_Mesa_Roads_Rails\"\n",
    "\n",
    "if LOAD_DATASETS:\n",
    "    # Load in preprocessed and presplit data\n",
    "    (\n",
    "        images_train,\n",
    "        masks_train,\n",
    "        images_test,\n",
    "        masks_test,\n",
    "        images_validation,\n",
    "        masks_validation,\n",
    "        ) = load_datasets(load_dir=load_dir)\n",
    "else:\n",
    "    # Split out the training data\n",
    "    (\n",
    "        images_train,\n",
    "        images_test_and_val,\n",
    "        masks_train,\n",
    "        masks_test_and_val,\n",
    "        names_train,\n",
    "        names_test_and_val,\n",
    "    ) = train_test_split(\n",
    "        preprocessed_images,\n",
    "        preprocessed_masks,\n",
    "        image_names,\n",
    "        test_size=0.2,\n",
    "        random_state=seed_value,\n",
    "    )\n",
    "\n",
    "    # Split the remaining data into validation and test\n",
    "    (\n",
    "        images_validation,\n",
    "        images_test,\n",
    "        masks_validation,\n",
    "        masks_test,\n",
    "        names_validation,\n",
    "        names_test,\n",
    "    ) = train_test_split(\n",
    "        images_test_and_val,\n",
    "        masks_test_and_val,\n",
    "        names_test_and_val,\n",
    "        test_size=0.5,\n",
    "        random_state=seed_value,\n",
    "    )\n",
    "\n",
    "# If SAVE_DATASETS is True, then save the data to the save_dir\n",
    "if SAVE_DATASETS:\n",
    "    save_datasets(\n",
    "        training=(images_train, masks_train),\n",
    "        test=(images_test, masks_test),\n",
    "        validation=(images_validation, masks_validation),\n",
    "        save_dir=save_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train the U-Net Model\n",
    "\n",
    "In the following three code cells, we will configure and build the U-Net model, assign training parameters, and train the model.\n",
    "\n",
    "### Step 10: Configure and Build the U-Net Model\n",
    "\n",
    "We first configure our model parameters. \n",
    "\n",
    "The first parameter is the optimizer, which is the optimization function used to update the model's weights and biases to minimize the loss function. In short it determines how the model learns. In our example we have chosen to use the `Adam` optimizer with a specific `learning_rate`. Note that, as with most machine learning libraries the function is already built into the library, in this case Tensorflow. Details can be found [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)\n",
    "\n",
    "THe next parameter we assign are the metrics. The default metric is `[\"accuracy\"]` which is a simple metric that shows how often the prediction masks in training, match the labels. Instead we have chosen to use two custom metrics defined in `unet.ipynb`:\n",
    "- `iou_metric`: Intersection Over Union (IOU) measures the overlap between the predicted mask classes, and the true mask classes.\n",
    "- `dice_coeff`: Dice ceofficient also measures the overlap between the predicted mask classes and the true mask classes, but it weights true positives by a factor of 2. \n",
    "\n",
    "Both of the above metrics are custom defined functions whose details can be found in `unet.ipynb` or [here](https://github.com/cna-iguide/iGuide_SpatialAI_2024).It is important to note, that because these are custom metrics, rather than Tensorflows built in metrics like `\"accuracy\"`, we must provide a dictionary of those custom objects, if we wish to save and load our model for future use, or after training. We do that with `custom_objects`, and you can see where this is relevant in [Step 14](#step-14-optionally-load-a-saved-model) \n",
    "\n",
    "Next we can call our custom `unet_model` function defined in `unet.ipynb`. This is a 'functional' implementation of a Tensorflow `Model` class, which means that we define a function that creates the `Model` class object defined by Tensorflow, which in turn means we have access to Tensorflow's `Model` class properties like `.summary()`. You can find more details on how to construct a \"functional\" model or how to subclass the `Model` class [here](https://www.tensorflow.org/api_docs/python/tf/keras/Model)\n",
    "\n",
    "In our example the images are of shape 256 x 256, and they are 3 channel RGB images. \n",
    "The `num_classes` corresponds to the `num_classes` output by `preprocess_images_and_masks` in [Step 8](#step-8-preprocess-the-data).\n",
    "We have set the optional settings such as:\n",
    "- `num_blocks`: setting the depth of the U-Net to `4`, which means that there are 4 encoder blocks, and their corresponding 4 decoder blocks. \n",
    "\n",
    "Finally `model.summary()` prints a summary of the U-Net model structure, including each layer in each block, the layer type, the output shape of each layer, the number of parameters in each layer, and the layer's connection to other layers. Please note, that the output shape always starts with `None`. This is the batch size of the data, and this is not fixed nor set in our model. \n",
    "\n",
    "More details on the U-Net model and it's structure can be found in `unet.ipynb` or [here](https://github.com/cna-iguide/iGuide_SpatialAI_2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the U-Net\n",
    "# Specify the details of the optimizer\n",
    "optimizer = optimizers.Adam(learning_rate=1e-4) # \"adam\", \n",
    "\n",
    "# Create the model\n",
    "# NOTE: We have chosen to use some custom metrics iou_metric and dice_coeff. The default value is [\"accuracy\"].\n",
    "metrics = [iou_metric, dice_coeff]\n",
    "custom_objects = {\"iou_metric\": iou_metric, \"dice_coeff\": dice_coeff}\n",
    "model = unet_model(\n",
    "    input_shape=(256, 256, 3),\n",
    "    num_classes=num_classes,\n",
    "    num_blocks=4,\n",
    "    optimizer=optimizer,\n",
    "    metrics=metrics, # [\"accuracy\"]\n",
    ")\n",
    "# Get a print out of the model structure.\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Configure Training Parameters\n",
    "\n",
    "Next we configure the training parameters we'd like to use for our model. \n",
    "\n",
    "We first set three model callbacks, which are built in parameters that we can use from Tensorflow. We implement a learning rate scheduler, using `ReduceLROnPlateau` which reduces the learning rate of our model by a factor of 0.5 when the validation loss (`\"val_loss\"`) plateaus after 5 epochs. We similarly set the model to stop training early using `EarlyStopping`, when the validation loss stops improving after 10 epochs, and we set it ot return the best weights when it does stop early. We also also use `ModelCheckpoint` to set the model to save the best weights based on the performance of the validation loss, at every epoch (by default). The details of these three callbacks, and others can be found [here](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/).\n",
    "\n",
    "We also set: \n",
    "- `epochs`: the number of times the entire data set is passed throuhg the model. More epochs means that the model is able to better train to the data it is given, but an increase in epochs also risks the model overfitting to the training dataset. \n",
    "- `batch_size`: the number of data samples processed before weights are updated; the 'batching' of the data. A smaller batch size updates the model more frequently and require less memory, but it is also noisier during training. \n",
    "- `model_verbosity`: The amount and type of information displayed during training, and the frequency with which the information is displayed. Setting verbsoity to `1` gives us the progress over each epoch, including approximate completion time of training for that epoch, and a detailed printout, depending on the verbosity of our callbacks and metrics, of the training parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign specific training parameters\n",
    "# Learning rate scheduler to adjust the learning rate if the validation loss is not improving.\n",
    "learning_rate_scheduler = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6, verbose=1\n",
    ")\n",
    "# Stop the model early, and keep the best results, if the validation loss is not improving.\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
    ")\n",
    "# Save the best model as determined by the validation loss. The default save frequency is every epoch.\n",
    "checkpoint = callbacks.ModelCheckpoint(\n",
    "    mode1_path, save_best_only=True, monitor=\"val_loss\"\n",
    ")\n",
    "\n",
    "# Gather the model callbacks into a single variable\n",
    "model_callbacks = [learning_rate_scheduler, early_stop, checkpoint]\n",
    "\n",
    "# Assign the number of epochs. \n",
    "epochs = 25\n",
    "batch_size = 8\n",
    "model_verbosity = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Train the Model\n",
    "\n",
    "We are finally ready to train our model! We pass through the `model` from [Step 10](#step-10-configure-and-build-the-u-net-model), our parameters from [Step 11](#step-11-configure-training-parameters), and our training and validation data from [Step 9](#step-9-split-the-data). Again please note that we are able to call `model.fit` because our model is a Tensorflow `Model` class and has the property `.fit`. You can find the full details on the Tensorflow `Model` class [here](https://www.tensorflow.org/api_docs/python/tf/keras/Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model_fit = model.fit(\n",
    "    images_train,\n",
    "    masks_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(images_validation, masks_validation),\n",
    "    callbacks=model_callbacks,\n",
    "    verbose=model_verbosity,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "In this final section we have four code cells for model evaluation: we will plot the training and validation losses, optionally load a saved model, test the trained model, optionally save the predicted masks, and optionally copy and save any additional files that we want included with our predicted masks. \n",
    "\n",
    "### Step 13: Plot Training Losses\n",
    "\n",
    "In this step we call a custom function `plot_losses`, to plot the training and validation losses of the model over the epochs. The details of this function can be found in `unet.ipynb` and [here](https://github.com/cna-iguide/iGuide_SpatialAI_2024). From this plto we can determine if the model has been fully trained (has the loss begun to plateau) and whether the model is being overfit to the training data (the validation loss gets larger while the training loss plateaus or decreases).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View model losses\n",
    "plot_losses(model_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 14: Optionally Load a Saved Model\n",
    "\n",
    "In this step, we can optionally load a previously saved model. This is useful if we have a model that we would like to train on a new set of data, if our training was previously interrupted, or if we'd like to test a model on a new data set. Recall from [Step 10](#step-10-configure-and-build-the-u-net-model) we used custom metrics and saved them to a dictionary `custom_objects`. Because we used custom metrics, we must include that dictionary when loading our model. Tensorflow does not save custom objects (in our case only custom metrics, but you can also define custom callbacks, custom optimizers, etc.) when saving the model, but becasue they are referenced in the model, they must be defined when loading the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional loading of a model\n",
    "# NOTE: If you have use custom metrics, you must explicitly load them back in with the model. \n",
    "LOAD_MODEL = False\n",
    "if LOAD_MODEL:\n",
    "    model = tf.keras.models.load_model(\n",
    "        model_path, custom_objects=custom_objects\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 15: Test the Trained Model\n",
    "\n",
    "In this step we can test how our model performs on our test data set generated in [Step 9](#step-9-split-the-data). \n",
    "We first ensure that our model is fixed, so that it will not update by setting `model.trainable = False`.\n",
    "We can use the number of classes printed out in [Step 8](#step-8-preprocess-the-data), to assign `class_labels` if we'd like, to use in the figures of the masks. Similarly we can specify the color map to use when displaying the masks. The color maps come from the `matplotlib` library and can be found [here](https://matplotlib.org/stable/users/explain/colors/colormaps.html).\n",
    "\n",
    "We then call the custom function `test_unet`. This is defined in `unet.ipynb` and additional details can be found there or [here](https://github.com/cna-iguide/iGuide_SpatialAI_2024). We feed in the model, the test images and masks, and the number of classes. \n",
    "We have set the optional settings such as:\n",
    "- `threshold`: The threshold to use on binary data when separating it into foreground and background. The default is 0.5, and it is an output of the `preprocess_images_and_masks` in [Step 8](#step-8-preprocess-the-data), as a precaution, since in the preprocessing, as previously discussed, we may convert multi-class masks to binary masks where appropriate. \n",
    "- `image_names`: The names of the images to display. We set this value in [Step 9](#step-9-split-the-data).\n",
    "- `display_count`: The number of predicted masks to display. Because the figure displays the image, the true mask, and the predicted mask, you should keep this value relatively small to avoid clutter. \n",
    "- `class_labels`: The labels for the classes. Note that if this is incorrect, the values will be overriden with just the class index.\n",
    "- `class_color_map`: the color map that we specified using the `matplotlib` library's color maps. \n",
    "\n",
    "The `test_unet` function will output Precision, Recall, and F1 Scores for each class, or single values in the binary mask classes case. These three metrics are defined in  `unet.ipynb` but are included here for clarity as well:\n",
    "- Precision: Calculates the precision, which is the ratio of correctly predicted positive observations to the total predicted positives. It is a measure of the accuracy of the positive predictions.\n",
    "    - Precision = true_positives / (true_positives + false_positives)\n",
    "- Recall: Calculates the recall, also known as sensitivity or true positive rate, which is the ratio of correctly predicted positive observations to all the actual positives. It is a measure of how well the model can capture positive instances.\n",
    "    - Recall = true_positives / (true_positives + false_negatives)\n",
    "- F1 Score: Calculates the F1 score, which is the weighted average of precision and recall. It considers both false positives and false negatives and is useful when you need a balance between precision and recall.\n",
    "    - F1 Score = 2 * (Precision * Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the model won't change now that it has finished training\n",
    "model.trainable = False\n",
    "# Test the model\n",
    "class_labels = {0: \"Background\", 1: \"Low to Medium Risk\", 2: \"High Risk\", 3: \" Very High Risk\", 4: \"Extreme Risk\"}\n",
    "class_color_map = \"viridis\"\n",
    "predicted_masks = test_unet(\n",
    "    model,\n",
    "    images_test,\n",
    "    masks_test,\n",
    "    num_classes,\n",
    "    threshold=threshold,\n",
    "    image_names=names_test,\n",
    "    display_count=3,\n",
    "    class_labels=class_labels,\n",
    "    class_color_map=class_color_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 16: Optionally Save the Predicted Masks\n",
    "\n",
    "In this step, we have included the option to save the predicted masks as images in the users format of choice, though preferably `tif` or `png`. \n",
    "\n",
    "The `save_predicted_masks` is a custom function whose details can be found in `unet.ipynb` or [here](https://github.com/cna-iguide/iGuide_SpatialAI_2024).\n",
    "\n",
    "We have set the optional settings such as:\n",
    "- `mask_names`: the names of the test masks which we generated in [Step 9](#step-9-split-the-data).\n",
    "- `file_ext`: the file extension (or format) we would like to save the predicted masks in. The default is `tif`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predicted masks\n",
    "SAVE_PREDICTED_MASKS = True\n",
    "predicted_masks_dir = Path(f\"{model_dir}/predicted_masks/\")\n",
    "if SAVE_PREDICTED_MASKS:\n",
    "    save_predicted_masks(\n",
    "        predicted_masks, predicted_masks_dir, mask_names=names_test, file_ext=\"tif\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 17: Optionally Save Addtional Files\n",
    "\n",
    "In this step, we have included the option to copy files from an existing folder, to another folder. The primary intended use is in the cases of `.tif` masks which have an associated `.tfw` file that contains necessary geospatial information. We may want to connect the geospatial information to the predicted masks. Since this data does not change with the prediction, we can copy the existing files to the folder where the predicted masks were saved. This function can similarly be used to save a subset of the images or masks with the predicted masks.  \n",
    "\n",
    "The `save_associated_files` is a custom function whose details can be found in `utils.ipynb` or [here](https://github.com/cna-iguide/iGuide_SpatialAI_2024).\n",
    "\n",
    "We have set the optional settings such as:\n",
    "- `file_ext`: the file extension (or format) of the files we would like to copy. The default is `tfw`.\n",
    "- `file_names`: the names of the files we would like to copy. This can be either the trimmed file names (in which case `names_map` should also be provided), or the untrimmed file names.\n",
    "- `names_map`: the mapping between the original file names and the trimmed file names. This is output by the `load_images_and_masks` function in [Step 5](#step-5-load-the-images-and-masks). \n",
    "\n",
    "Note: If `file_names` is not provided, then all files of the given `file_ext` from the original folder will be copied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save files associated with the predicted masks, or other files of interest. \n",
    "save_associated_files(\n",
    "    masks_dir,\n",
    "    predicted_masks_dir,\n",
    "    file_ext=\"tfw\",\n",
    "    file_names=names_test,\n",
    "    names_map=names_map,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatialenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
